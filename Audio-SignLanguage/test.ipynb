{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AMMU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please say something...\n",
      "Could not understand audio\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import cv2\n",
    "import imageio\n",
    "\n",
    "# Initialize spaCy and NLTK\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "#Capture Audio and Transcribe\n",
    "def transcribe_audio():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Please say something...\")\n",
    "        audio = recognizer.listen(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            print(\"Transcription: \", text)\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio\")\n",
    "            return None\n",
    "        except sr.RequestError:\n",
    "            print(\"Could not request results; check your network connection\")\n",
    "            return None\n",
    "\n",
    "# Step 2: Process Text with NLP\n",
    "# def process_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     processed_text = []\n",
    "#     for token in doc:\n",
    "#         # Using wordnet to get synonyms (or potentially sign-friendly alternatives)\n",
    "#         synonyms = wn.synsets(token.text)\n",
    "#         if synonyms:\n",
    "#             # Take the first synonym as an example\n",
    "#             processed_text.append(synonyms[0].lemma_names()[0])\n",
    "#         else:\n",
    "#             processed_text.append(token.text)\n",
    "#     return \" \".join(processed_text)\n",
    "def process_text(text):\n",
    "    doc = nlp(text)\n",
    "    processed_text = []\n",
    "    \n",
    "    # Custom mapping for specific words to bypass synonym replacement\n",
    "    custom_mappings = {\n",
    "        \"I\": \"I\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"no\": \"no\",\n",
    "        \"yes\": \"yes\"\n",
    "        # Add more as needed\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        word = token.text.lower()  # Normalize to lowercase for matching\n",
    "        \n",
    "        # Skip synonym replacement for certain parts of speech or custom mappings\n",
    "        if word in custom_mappings:\n",
    "            processed_text.append(custom_mappings[word])\n",
    "        elif token.pos_ in {\"PRON\", \"AUX\", \"PART\"}:  # Skip pronouns, auxiliaries, and particles\n",
    "            processed_text.append(word)\n",
    "        else:\n",
    "            # Attempt to get a synonym if available\n",
    "            synonyms = wn.synsets(word)\n",
    "            if synonyms:\n",
    "                processed_text.append(synonyms[0].lemma_names()[0])  # First lemma as default\n",
    "            else:\n",
    "                processed_text.append(word)\n",
    "    \n",
    "    return \" \".join(processed_text)\n",
    "\n",
    "\n",
    "def display_sign_gif(word):\n",
    "    # Dictionary of words mapped to GIF file paths\n",
    "    sign_gifs = {\n",
    "        \"hello\": \"static/gifs/hello.gif\",\n",
    "        \"thank you\": \"static/gifs/thank_you.gif\",\n",
    "        \"thank\": \"static/gifs/thank_you.gif\",\n",
    "        \"bye\": \"static/gifs/good_bye.gif\",\n",
    "        \"goodbye\": \"static/gifs/good_bye.gif\",\n",
    "        \"broke\":\"static/gifs/broken.gif\",\n",
    "        \"broken\":\"static/gifs/broken.gif\",\n",
    "        \"love\":\"static/gifs/love.jpeg\",\n",
    "        \"I do not know\":\"static/gifs/i_dont_know.gif\",\n",
    "        \"cat\":\"static/gifs/cat.webp\",\n",
    "        \"help\":\"static/gifs/help.gif\",\n",
    "        \"sorry\":\"static/gifs/sorry.jpeg\",\n",
    "        \"dead\":\"static/gifs/dead.gif\",\n",
    "        \"morning\":\"static/gifs/morning.gif\",\n",
    "        \"happy\":\"static/gifs/happy.gif\",\n",
    "        \"nothing\":\"static/gifs/nothing.gif\",\n",
    "        \"stop\":\"static/gifs/stop.gif\",\n",
    "        \"hot\":\"static/gifs/hot.gif\",\n",
    "        \"dancing\":\"static/gifs/dancing.gif\",\n",
    "        \"dinner\":\"static/gifs/dinner.gif\",\n",
    "        \"weather\":\"static/gifs/weather.gif\",\n",
    "        \"pain\":\"static/gifs/pain.gif\",\n",
    "        \"sad\":\"static/gifs/sad.gif\",\n",
    "        \"tomarrow\":\"static/gifs/tomorrow.gif\",\n",
    "        \"weekend\":\"static/gifs/weekend.gif\",\n",
    "        \"exhausted\":\"static/gifs/exhausted.gif\",\n",
    "        \"lazy\":\"static/gifs/lazy.gif\",\n",
    "        \"war\":\"static/gifs/war.gif\",\n",
    "        \"summer\":\"static/gifs/summer.gif\",\n",
    "        \"cool\":\"static/gifs/cool.gif\",\n",
    "        \"breathing\":\"static/gifs/breathing.gif\",\n",
    "        \"lucky\":\"static/gifs/lucky.gif\",\n",
    "        \"play\":\"static/gifs/play.gif\",\n",
    "        \"cool\":\"static/gifs/cool.gif\",\n",
    "        \"sorry\":\"static/gifs/sorry.gif\",\n",
    "        \"hungry\":\"static/gifs/hungary.gif\",\n",
    "        \"milk\":\"static/gifs/milk.gif\",\n",
    "        \"goodnight\":\"static/gifs/goodnight.gif\",\n",
    "        \"sick\":\"static/gifs/sick.gif\",\n",
    "        \"scary\":\"static/gifs/scary.gif\",\n",
    "        \"birthday\":\"static/gifs/birthday.gif\",\n",
    "        \"annoyed\":\"static/gifs/annoyed.gif\",\n",
    "        \"fun\":\"static/gifs/fun.gif\",\n",
    "        \"evening\":\"static/gifs/evening.gif\"\n",
    "    \n",
    "\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # First, try to match the whole phrase in the dictionary\n",
    "  \n",
    "    \n",
    "    if word in sign_gifs:\n",
    "        gif_path = sign_gifs[word]\n",
    "        gif = imageio.mimread(gif_path)  # Load GIF frames\n",
    "        print(f\"Displaying GIF for '{word}' with {len(gif)} frames\")\n",
    "        \n",
    "        for i, frame in enumerate(gif):\n",
    "            img = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imshow(f\"Sign for '{word}'\", img)\n",
    "            if cv2.waitKey(200) & 0xFF == ord('q'):  # Display each frame, break on 'q' key\n",
    "                break\n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        print(f\"GIF for '{word}' not found.\")\n",
    "\n",
    "def main():\n",
    "    # Step 1: Transcribe Audio\n",
    "    text = transcribe_audio()\n",
    "    if text:\n",
    "        # Step 2: Process Text with NLP\n",
    "        simplified_text = process_text(text)\n",
    "        print(\"Processed Text:\", simplified_text)\n",
    "\n",
    "        # Step 3: Display Sign Language GIF for the entire phrase\n",
    "        display_sign_gif(simplified_text)  # Pass full text to try phrase matching first\n",
    "\n",
    "# Main Program Flow\n",
    "def main():\n",
    "    # Step 1: Transcribe Audio\n",
    "    text = transcribe_audio()\n",
    "    if text:\n",
    "        # Step 2: Process Text with NLP\n",
    "        simplified_text = process_text(text)\n",
    "        print(\"Processed Text:\", simplified_text)\n",
    "\n",
    "        # Step 3: Display Sign Language Images\n",
    "        for word in simplified_text.split():\n",
    "            display_sign_gif(word)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
